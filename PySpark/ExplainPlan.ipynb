{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/13 22:23:38 WARN Utils: Your hostname, SAURAVs-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.29.194 instead (on interface en0)\n",
      "23/12/13 22:23:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/13 22:23:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('explain_plan').getOrCreate()\n",
    "# spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = spark.range(1, 1000000)\n",
    "times5 = n.selectExpr(\"id * 5 as id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  5|\n",
      "| 10|\n",
      "| 15|\n",
      "+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "times5.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [(id#0L * 5) AS id#2L]\n",
      "+- *(1) Range (1, 1000000, step=1, splits=8)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "times5.explain(mode = 'simple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [('id * 5) AS id#2]\n",
      "+- Range (1, 1000000, step=1, splits=Some(8))\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: bigint\n",
      "Project [(id#0L * cast(5 as bigint)) AS id#2L]\n",
      "+- Range (1, 1000000, step=1, splits=Some(8))\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [(id#0L * 5) AS id#2L]\n",
      "+- Range (1, 1000000, step=1, splits=Some(8))\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [(id#0L * 5) AS id#2L]\n",
      "+- *(1) Range (1, 1000000, step=1, splits=8)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "times5.explain(mode = 'extended') #extended=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Exchange RoundRobinPartitioning(7), REPARTITION_BY_NUM, [plan_id=50]\n",
      "   +- Range (1, 1000000, step=2, splits=8)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "m = spark.range(1, 1000000, 2)\n",
    "split7 = m.repartition(7)\n",
    "split7.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|    id|\n",
      "+------+\n",
      "|106581|\n",
      "| 99177|\n",
      "| 64689|\n",
      "+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "split7.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Range (1, 1000000, step=2, splits=8)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "m.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[], functions=[sum(id#83L)])\n",
      "   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=226]\n",
      "      +- HashAggregate(keys=[], functions=[partial_sum(id#83L)])\n",
      "         +- Project [id#83L]\n",
      "            +- BroadcastHashJoin [id#83L], [id#77L], Inner, BuildRight, false\n",
      "               :- Project [(id#75L * 5) AS id#83L]\n",
      "               :  +- Exchange RoundRobinPartitioning(7), REPARTITION_BY_NUM, [plan_id=212]\n",
      "               :     +- Range (1, 100, step=1, splits=8)\n",
      "               +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=221]\n",
      "                  +- Exchange RoundRobinPartitioning(9), REPARTITION_BY_NUM, [plan_id=215]\n",
      "                     +- Range (1, 100, step=2, splits=8)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds1 = spark.range(1, 100)\n",
    "ds2 = spark.range(1, 100, 2)\n",
    "ds3 = ds1.repartition(7)\n",
    "ds4 = ds2.repartition(9)\n",
    "ds5 = ds3.selectExpr(\"id * 5 as id\")\n",
    "joined = ds5.join(ds4, \"id\")\n",
    "sum = joined.selectExpr(\"sum(id)\")\n",
    "sum.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [unresolvedalias('sum('id), Some(org.apache.spark.sql.Column$$Lambda$3080/0x00000008003cae78@ec47cd8))]\n",
      "+- Project [id#41L]\n",
      "   +- Join Inner, (id#41L = id#35L)\n",
      "      :- Project [(id#33L * cast(5 as bigint)) AS id#41L]\n",
      "      :  +- Repartition 7, true\n",
      "      :     +- Range (1, 1000000, step=1, splits=Some(8))\n",
      "      +- Repartition 9, true\n",
      "         +- Range (1, 1000000, step=2, splits=Some(8))\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "sum(id): bigint\n",
      "Aggregate [sum(id#41L) AS sum(id)#45L]\n",
      "+- Project [id#41L]\n",
      "   +- Join Inner, (id#41L = id#35L)\n",
      "      :- Project [(id#33L * cast(5 as bigint)) AS id#41L]\n",
      "      :  +- Repartition 7, true\n",
      "      :     +- Range (1, 1000000, step=1, splits=Some(8))\n",
      "      +- Repartition 9, true\n",
      "         +- Range (1, 1000000, step=2, splits=Some(8))\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [sum(id#41L) AS sum(id)#45L]\n",
      "+- Project [id#41L]\n",
      "   +- Join Inner, (id#41L = id#35L)\n",
      "      :- Project [(id#33L * 5) AS id#41L]\n",
      "      :  +- Repartition 7, true\n",
      "      :     +- Range (1, 1000000, step=1, splits=Some(8))\n",
      "      +- Repartition 9, true\n",
      "         +- Range (1, 1000000, step=2, splits=Some(8))\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[], functions=[sum(id#41L)], output=[sum(id)#45L])\n",
      "   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=142]\n",
      "      +- HashAggregate(keys=[], functions=[partial_sum(id#41L)], output=[sum#48L])\n",
      "         +- Project [id#41L]\n",
      "            +- BroadcastHashJoin [id#41L], [id#35L], Inner, BuildRight, false\n",
      "               :- Project [(id#33L * 5) AS id#41L]\n",
      "               :  +- Exchange RoundRobinPartitioning(7), REPARTITION_BY_NUM, [plan_id=128]\n",
      "               :     +- Range (1, 1000000, step=1, splits=8)\n",
      "               +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=137]\n",
      "                  +- Exchange RoundRobinPartitioning(9), REPARTITION_BY_NUM, [plan_id=131]\n",
      "                     +- Range (1, 1000000, step=2, splits=8)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sum.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (12)\n",
      "+- HashAggregate (11)\n",
      "   +- Exchange (10)\n",
      "      +- HashAggregate (9)\n",
      "         +- Project (8)\n",
      "            +- BroadcastHashJoin Inner BuildRight (7)\n",
      "               :- Project (3)\n",
      "               :  +- Exchange (2)\n",
      "               :     +- Range (1)\n",
      "               +- BroadcastExchange (6)\n",
      "                  +- Exchange (5)\n",
      "                     +- Range (4)\n",
      "\n",
      "\n",
      "(1) Range\n",
      "Output [1]: [id#75L]\n",
      "Arguments: Range (1, 100, step=1, splits=Some(8))\n",
      "\n",
      "(2) Exchange\n",
      "Input [1]: [id#75L]\n",
      "Arguments: RoundRobinPartitioning(7), REPARTITION_BY_NUM, [plan_id=212]\n",
      "\n",
      "(3) Project\n",
      "Output [1]: [(id#75L * 5) AS id#83L]\n",
      "Input [1]: [id#75L]\n",
      "\n",
      "(4) Range\n",
      "Output [1]: [id#77L]\n",
      "Arguments: Range (1, 100, step=2, splits=Some(8))\n",
      "\n",
      "(5) Exchange\n",
      "Input [1]: [id#77L]\n",
      "Arguments: RoundRobinPartitioning(9), REPARTITION_BY_NUM, [plan_id=215]\n",
      "\n",
      "(6) BroadcastExchange\n",
      "Input [1]: [id#77L]\n",
      "Arguments: HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=221]\n",
      "\n",
      "(7) BroadcastHashJoin\n",
      "Left keys [1]: [id#83L]\n",
      "Right keys [1]: [id#77L]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(8) Project\n",
      "Output [1]: [id#83L]\n",
      "Input [2]: [id#83L, id#77L]\n",
      "\n",
      "(9) HashAggregate\n",
      "Input [1]: [id#83L]\n",
      "Keys: []\n",
      "Functions [1]: [partial_sum(id#83L)]\n",
      "Aggregate Attributes [1]: [sum#89L]\n",
      "Results [1]: [sum#90L]\n",
      "\n",
      "(10) Exchange\n",
      "Input [1]: [sum#90L]\n",
      "Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=226]\n",
      "\n",
      "(11) HashAggregate\n",
      "Input [1]: [sum#90L]\n",
      "Keys: []\n",
      "Functions [1]: [sum(id#83L)]\n",
      "Aggregate Attributes [1]: [sum(id#83L)#86L]\n",
      "Results [1]: [sum(id#83L)#86L AS sum(id)#87L]\n",
      "\n",
      "(12) AdaptiveSparkPlan\n",
      "Output [1]: [sum(id)#87L]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sum.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = [\n",
    "    ['1', 'Java', '20000'],\n",
    "    ['2', 'Python', '30000'],\n",
    "    ['3', 'Scala', '3000']\n",
    "]\n",
    "schema1 = ['id', 'language', 'fees']\n",
    "languages = spark.createDataFrame(data1, schema1)\n",
    "languages.createOrReplaceTempView(\"languages\")\n",
    "\n",
    "data2 = [\n",
    "    ['1', 'StudentA'],\n",
    "    ['2', 'StudentB'],\n",
    "    ['3', 'StudentA'],\n",
    "    ['1', 'StudentC'],\n",
    "    ['3', 'StudentB']\n",
    "]\n",
    "schema2 = ['language_id', 'studentName']\n",
    "students = spark.createDataFrame(data2, schema2)\n",
    "students.createOrReplaceTempView(\"students\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(\"\"\"SELECT students.studentName, SUM(students.language_id) as sum\n",
    "               FROM students\n",
    "               INNER JOIN languages\n",
    "               ON students.language_id = languages.id\n",
    "               WHERE students.studentName = 'StudentC' \n",
    "               group By students.studentName\n",
    "               \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+\n",
      "|studentName|sum|\n",
      "+-----------+---+\n",
      "|   StudentC|1.0|\n",
      "+-----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['students.studentName], ['students.studentName, 'SUM('students.language_id) AS sum#294]\n",
      "+- 'Filter ('students.studentName = StudentC)\n",
      "   +- 'Join Inner, ('students.language_id = 'languages.id)\n",
      "      :- 'UnresolvedRelation [students], [], false\n",
      "      +- 'UnresolvedRelation [languages], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "studentName: string, sum: double\n",
      "Aggregate [studentName#186], [studentName#186, sum(cast(language_id#185 as double)) AS sum#294]\n",
      "+- Filter (studentName#186 = StudentC)\n",
      "   +- Join Inner, (language_id#185 = id#179)\n",
      "      :- SubqueryAlias students\n",
      "      :  +- View (`students`, [language_id#185,studentName#186])\n",
      "      :     +- LogicalRDD [language_id#185, studentName#186], false\n",
      "      +- SubqueryAlias languages\n",
      "         +- View (`languages`, [id#179,language#180,fees#181])\n",
      "            +- LogicalRDD [id#179, language#180, fees#181], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [studentName#186], [studentName#186, sum(cast(language_id#185 as double)) AS sum#294]\n",
      "+- Project [language_id#185, studentName#186]\n",
      "   +- Join Inner, (language_id#185 = id#179)\n",
      "      :- Filter ((isnotnull(studentName#186) AND (studentName#186 = StudentC)) AND isnotnull(language_id#185))\n",
      "      :  +- LogicalRDD [language_id#185, studentName#186], false\n",
      "      +- Project [id#179]\n",
      "         +- Filter isnotnull(id#179)\n",
      "            +- LogicalRDD [id#179, language#180, fees#181], false\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[studentName#186], functions=[sum(cast(language_id#185 as double))], output=[studentName#186, sum#294])\n",
      "   +- Exchange hashpartitioning(studentName#186, 200), ENSURE_REQUIREMENTS, [plan_id=1841]\n",
      "      +- HashAggregate(keys=[studentName#186], functions=[partial_sum(cast(language_id#185 as double))], output=[studentName#186, sum#305])\n",
      "         +- Project [language_id#185, studentName#186]\n",
      "            +- SortMergeJoin [language_id#185], [id#179], Inner\n",
      "               :- Sort [language_id#185 ASC NULLS FIRST], false, 0\n",
      "               :  +- Exchange hashpartitioning(language_id#185, 200), ENSURE_REQUIREMENTS, [plan_id=1833]\n",
      "               :     +- Filter ((isnotnull(studentName#186) AND (studentName#186 = StudentC)) AND isnotnull(language_id#185))\n",
      "               :        +- Scan ExistingRDD[language_id#185,studentName#186]\n",
      "               +- Sort [id#179 ASC NULLS FIRST], false, 0\n",
      "                  +- Exchange hashpartitioning(id#179, 200), ENSURE_REQUIREMENTS, [plan_id=1834]\n",
      "                     +- Project [id#179]\n",
      "                        +- Filter isnotnull(id#179)\n",
      "                           +- Scan ExistingRDD[id#179,language#180,fees#181]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.explain(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
